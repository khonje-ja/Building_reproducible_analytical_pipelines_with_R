---
title: "Housing in Luxembourg"
author: "James"
date: "2023-07-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



We are going to download data about house prices in Luxembourg. Luxembourg is a little Western European country the author hails from that looks like a shoe and is about the size of .98 Rhode Islands. 

The House Price Index (HPI) measures inflation in the residential property market. The HPI captures price changes of all types of dwellings purchased by households (flats, detached houses, terraced houses, etc.). Only transacted dwellings are considered, self-build dwellings are excluded. The land component of the dwelling is included.

our goal is to:

Get data trapped inside an Excel file into a neat data frame;
Convert nominal to real prices using a simple method;
Make some tables and plots and call it a day (for now).

Each sheet contains a dataset with the following columns:

Commune: the commune (the smallest administrative division of territory);
Nombre d’offres: the total number of selling offers;
Prix moyen annoncé en Euros courants: Average selling price in nominal Euros;
Prix moyen annoncé au m2 en Euros courants: Average selling price in square meters in nominal Euros.

## Loading packages
```{r}

library(pacman)
p_load(dplyr, purrr, readxl, stringr, janitor, knitr, pander)
```

## Download data

```{r}

# Book used: https://raps-with-r.dev/project_start.html

# The url below points to an Excel file hosted on the book’s github repository
# the code below downloads the data, and puts it in a data frame:

url <- "https://is.gd/1vvBAc"

raw_data <- tempfile(fileext = ".xlsx")

download.file(url, raw_data,
              method = "auto",
              mode = "wb")

sheets <- excel_sheets(raw_data)

read_clean <- function(..., sheet){
  read_excel(..., sheet = sheet) |>
    mutate(year = sheet)
}

raw_data <- map(
  sheets,
  ~read_clean(raw_data,
              skip = 10,
              sheet = .)
                   ) |>
  bind_rows() |>
  clean_names()

raw_data <- raw_data |>
  rename(
    locality = commune,
    n_offers = nombre_doffres,
    average_price_nominal_euros = prix_moyen_annonce_en_courant,
    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant,
    average_price_m2_nominal_euros = prix_moyen_annonce_au_m2_en_courant
  ) |>
  mutate(locality = str_trim(locality)) |>
  select(year, locality, n_offers, starts_with("average"))
```

```{r}
pander(head(raw_data, 10))
```
But there’s a problem: columns that should be of type numeric are of type character instead (average_price_nominal_euros and average_price_m2_nominal_euros). There’s also another issue, which you would eventually catch as you’ll explore the data: the naming of the communes is not consistent. Let’s take a look:


```{r}
raw_data |>
  filter(grepl("Luxembourg", locality)) |>
  count(locality)
```

We can see that the city of Luxembourg is spelled in two different ways. It’s the same with another commune, Pétange:



```{r}
raw_data |>
  filter(grepl("P.tange", locality)) |>
  count(locality)
```

So sometimes it is spelled correctly, with an “é”, sometimes not. Let’s write some code to correct both these issues:

```{r}
raw_data <- raw_data |>
  mutate(
    locality = ifelse(grepl("Luxembourg-Ville", locality),
                      "Luxembourg",
                      locality),
         locality = ifelse(grepl("P.tange", locality),
                           "Pétange",
                           locality)
         ) |>
  mutate(across(starts_with("average"),
         as.numeric))
```

Now this is interesting – converting the average columns to numeric resulted in some NA values. Let’s see what happened:


```{r}
pander(head(raw_data |>
  filter(is.na(average_price_nominal_euros))))
```
It turns out that there are no prices for certain communes, but that we also have some rows with garbage in there. Let’s go back to the raw data to see what this is about:

Always look at your data.

So it turns out that there are some rows that we need to remove. We can start by removing rows where locality is missing. Then we have a row where locality is equal to “Total d’offres”. This is simply the total of every offer from every commune. We could keep that in a separate data frame, or even remove it. The very last row states the source of the data and we can also remove it. Finally, in the screenshot above, we see another row that we don’t see in our filtered data frame: one where n_offers is missing. This row gives the national average for columns average_prince_nominal_euros and average_price_m2_nominal_euros. What we are going to do is create two datasets: one with data on communes, and the other on national prices. Let’s first remove the rows stating the sources:


```{r}
raw_data <- raw_data |>
  filter(!grepl("Source", locality))
```



Let’s now only keep the communes in our data:
```{r}
commune_level_data <- raw_data |>
    filter(!grepl("nationale|offres", locality),
           !is.na(locality))
```



And let’s create a dataset with the national data as well:

```{r}
country_level <- raw_data |>
  filter(grepl("nationale", locality)) |>
  select(-n_offers)

offers_country <- raw_data |>
  filter(grepl("Total d.offres", locality)) |>
  select(year, n_offers)

country_level_data <- full_join(country_level, offers_country) |>
  select(year, locality, n_offers, everything()) |>
  mutate(locality = "Grand-Duchy of Luxembourg")

pander(country_level_data)
```


Now the data looks clean, and we can start the actual analysis… or can we? Before proceeding, it would be nice to make sure that we got every commune in there. For this, we need a list of communes from Luxembourg. Thankfully, Wikipedia has such a list6.

Let’s scrape and save this list:

```{r}
current_communes <- "https://w.wiki/6nPu" |>
  rvest::read_html() |>
  rvest::html_table() |>
  purrr::pluck(1) |>
  janitor::clean_names()
```



We scrape the table from the Wikipedia page using {rvest}. rvest::html_table() returns a list of tables from the Wikipedia table, and then we use purrr::pluck() to keep the first table from the website, which is what we need (I made the calls to the packages explicit, because you might not be familiar with these packages). janitor::clean_names() transforms column names written for human eyes into machine-friendly names (for example Growth rate in % would be transformed to growth_rate_in_percent).

Let’s see if we have all the communes in our data:
```{r}
setdiff(unique(commune_level_data$locality),
        current_communes$commune)
```

We see many communes that are in our commune_level_data, but not in current_communes. There’s one obvious reason: differences in spelling, for example, “Kaerjeng” in our data, but “Käerjeng” in the table from Wikipedia. But there’s also a less obvious reason; since 2010, several communes have merged into new ones. So there are communes that are in our data in 2010 and 2011, but disappear from 2012 onwards. So we need to do several things: first, get a list of all existing communes from 2010 onwards, and then, harmonise spelling. Here again, we can use a list from Wikipedia:


```{r}
former_communes <- "https://w.wiki/_wFe7" |>
  rvest::read_html() |>
  rvest::html_table() |>
  purrr::pluck(3) |>
  janitor::clean_names() |>
  dplyr::filter(year_dissolved > 2009)

former_communes
```

As you can see, since 2010 many communes have merged to form new ones. We can now combine the list of current and former communes, as well as harmonise their names:

```{r}
communes <- unique(c(former_communes$name,
                     current_communes$commune))
```


# we need to rename some communes

# Different spelling of these communes between wikipedia and the data


```{r}
communes[which(communes == "Clemency")] <- "Clémency"
communes[which(communes == "Redange")] <- "Redange-sur-Attert"
communes[which(communes == "Erpeldange-sur-Sûre")] <- "Erpeldange"
communes[which(communes == "Luxembourg-City")] <- "Luxembourg"
communes[which(communes == "Käerjeng")] <- "Kaerjeng"
communes[which(communes == "Petange")] <- "Pétange"
```



Let’s run our test again:

```{r}

setdiff(unique(commune_level_data$locality),
        communes)
```




























































































































